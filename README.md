# RecSys-Challenge-2024-tetsuro731

- This repo stre the codes for the RecSys Challenge 2024 by tetsuro731 (Team Name: SBJ Partners).
- This repos was set to private during the competition, accessible only to team members.
- It will be public after the all competition events ends.

# Basic Strategy

- Typically, in recommendation systems, what is called two-step approach is used: candidate generation -> reranking stage.
- For this news recommendation task, since the list of articles displayed per impression is already provided, we focused only on the reranking phase.
- Generate effective features and feed them into LightGBM.

# Hyperparameter Tuning

- Used Optuna for hyperparameter tuning.

# How to run codes


- Run `RecSys2024_preprocess.ipynb` at first to preprocess features for input into LightGBM.
	- By setting parameters to train/valid/test, data for each period can be generated.
 	- It can be run on the small data provided officially.
  	- Running it on the full dataset takes time, so first validate with the small data and if it looks good, then run on the full dataset.

- `preprocess_create_embed.ipynb`
	- Code to generate embeddings to be used as features.
 	- In addition to the embeddings for each article_id provided by the official competition, it reads the embeddings for article titles, subtitles, and titles+subtitles created by yyama.
  	- The dimensionality is too high as-is, so it is reduced to 32 or 16 dimensions using PCA.
  	- Finally, 16 dimensions were adopted (higher dimensions increase accuracy but also computation time).

- `preprocess2_embed_similarity.ipynb`
	- Based on the n-dimensional embeddings generated by PCA, it calculates the similarity between users and articles.
 	- The user’s vector is the average of the vectors from their past n clicks.
  	- For the final submission, the user vector was the average of the past 5 clicks.

- `RecSyS2024_LGBM_train.ipynb`
	- The main code for training LightGBM.
 	- Performs rank learning and evaluates with nDCG@k (the main competition metric is AUC, but it has a somewhat unique calculation method).
  	- When the parameter is set to train, it trains on the train data and validates on the valid data.
  	- When the parameter is set to valid, it trains on the valid data to generate the model (the number of iterations is decided in advance by training on the train data).
  	- Usually run with learning rate=0.05 and early_stopping=20, but for the final submission on the full data, it was run with learning rate=0.03 and early_stopping=40.
  	- The seed is usually fixed at 42, but for the final submission, it was ensemble with seeds 52 and 62.
- `RecSyS2024_LGBM_test.ipynb`
	- Inference code, performs inference on test data using models generated during training.
 	- Inference results can be obtained from both train and valid models, and the final result is an ensemble of these in a 1:1 ratio.

# Training

- The number of features used for final sub was around 140.
- All the features could not load into memory at once, so 60% of the data was randomly sampled for training.
	- Memory usage exceeded 300GB on Google Colab Pro+, but still, 60% was the limit.
 	- Also Tried under-sampling of negative examples, but simply downsampling rows randomly yielded better accuracy.
	- Finally, the strategy was to use as much data as possible by ensemble with multiple seeds.

# Feature Engineering

## Article features

- Features like article categories and title lengths.
- NER clusters representing keywords for each article initially had very high importance but caused overfitting, so they were eventually removed.

## User features

- Information like user age and gender.
- These features had little effect individually and needed to be aggregated or combined for better performance.

## Article*User features

- For example, calculating the average age of users who viewed each article by grouping by user and comparing it to the actual user’s age to generate features.
- Conversely, grouping by article_id for each user to determine preferred articles and comparing them to actual article values.

## Embedding features

- Added cosine similarity as a feature by embedding users and articles.
- The official embeddings included information from the title, subtitle, and body.
- Created custom embeddings using only the title and subtitle, as these are what humans use to decide clicks.

## Session feature

- Features like the number of times the article appeared per session, the time elapsed since the last impression, etc.
- Included the time until the next session (though this uses future information and can be considered a leak, it was allowed in this competition).

## Impression time distribution

- Information on when the user impressed the article (“mean, std, count, etc.”).
- This includes information about when/how often the user impressed the article after the click time, thus containing a leak.

# Cross Validation
- The data for train, valid, and test are consecutive weekly data.
- Since the goal is to predict the test’s 7 days, local CV used 7 days of train data to predict 7 days of valid data.
- This local CV score correlated well with the Public LB, so it was used to improve the model.

# Ensemble

- The multiple models obtained were combined by calculating the weighted average of the scores to get the final result.
- For the final submission, the train/valid models and random seeds 42, 52, and 62 were ensemble with equal weights.
	- i.e. 6 models were used for ensemble for final sub 

